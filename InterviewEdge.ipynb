{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6026b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pyaudio\n",
    "from gtts import gTTS\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate a few functions\n",
    "\n",
    "def speak_question(question):\n",
    "    tts = gTTS(question)\n",
    "    tts.save(\"question.mp3\")\n",
    "    os.system(\"afplay question.mp3\")  # For MacOS, use \"start question.mp3\" on Windows      \n",
    "\n",
    "\n",
    "def listen_to_applicant(timeout=None, phrase_time_limit=30, ambient_duration=1.0, pause_threshold=1.8, non_speaking_duration=0.8, energy_threshold=None, show_prompts=True):\n",
    "    \"\"\"\n",
    "    Listens to the applicant and converts to text.\n",
    "\n",
    "    Parameters:\n",
    "    - timeout: max seconds to wait for the phrase to start (None = wait indefinitely)\n",
    "    - phrase_time_limit: max seconds for a single phrase (None = no limit). Increase to allow long answers.\n",
    "    - ambient_duration: seconds to adjust for ambient noise before listening\n",
    "    - pause_threshold: seconds of silence that indicate end of phrase (increase to wait longer)\n",
    "    - non_speaking_duration: how long to keep non-speaking buffer (increase to be more tolerant)\n",
    "    - energy_threshold: if set, fixes energy threshold (disable dynamic adjustment)\n",
    "    - show_prompts: print status messages\n",
    "    \"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    r.pause_threshold = pause_threshold\n",
    "    r.non_speaking_duration = non_speaking_duration\n",
    "\n",
    "    if energy_threshold is not None:\n",
    "        r.energy_threshold = energy_threshold\n",
    "        r.dynamic_energy_threshold = False\n",
    "    else:\n",
    "        r.dynamic_energy_threshold = True\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        if show_prompts:\n",
    "            print(\"Adjusting for ambient noise... (keep quiet)\")\n",
    "        r.adjust_for_ambient_noise(source, duration=ambient_duration)\n",
    "        if show_prompts:\n",
    "            print(\"Listening... (speak; pause to finish)\")\n",
    "\n",
    "        try:\n",
    "            audio = r.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)\n",
    "        except sr.WaitTimeoutError:\n",
    "            if show_prompts:\n",
    "                print(\"Listening timed out waiting for phrase to start.\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            if show_prompts:\n",
    "                print(f\"Error while listening: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    try:\n",
    "        text = r.recognize_google(audio)\n",
    "        if show_prompts:\n",
    "            print(f\"Applicant said: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        if show_prompts:\n",
    "            print(\"Could not understand audio\")\n",
    "        return \"\"\n",
    "    except sr.RequestError as e:\n",
    "        if show_prompts:\n",
    "            print(f\"Speech recognition service error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0696b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini API\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    gemini_model = genai.GenerativeModel('gemini-flash-latest') \n",
    "\n",
    "    # Re-create the fictitious job posting and CV files in the /content directory\n",
    "    job_posting_content = \"\"\"\n",
    "    Title: Machine Learning Engineer\n",
    "    Company: Google\n",
    "    Location: Mountain View, CA\n",
    "\n",
    "    About the Role:\n",
    "    Google is seeking a talented Machine Learning Engineer to join our innovative team. You will be responsible for designing, developing, and deploying scalable ML models that power various Google products. This role requires strong programming skills, a solid understanding of machine learning principles, and experience with large-scale data processing.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Design and implement machine learning models for specific applications.\n",
    "    - Develop and maintain robust data pipelines.\n",
    "    - Collaborate with cross-functional teams to integrate ML solutions.\n",
    "    - Evaluate and improve the performance of ML models.\n",
    "    - Stay up-to-date with the latest advancements in machine learning.\n",
    "\n",
    "    Qualifications:\n",
    "    - Bachelor's or Master's degree in Computer Science, related field, or equivalent practical experience.\n",
    "    - 3+ years of experience in machine learning or related field.\n",
    "    - Proficiency in Python and ML frameworks (e.g., TensorFlow, PyTorch).\n",
    "    - Experience with cloud platforms (e.g., Google Cloud, AWS).\n",
    "    - Strong problem-solving and communication skills.\n",
    "\n",
    "    Bonus Qualifications:\n",
    "    - Experience with natural language processing or computer vision.\n",
    "    - Publications in top-tier ML conferences.\n",
    "    \"\"\"\n",
    "\n",
    "    applicant_cv_content = \"\"\"\n",
    "    Name: Jane Doe\n",
    "    Email: jane.doe@email.com\n",
    "    Phone: 555-123-4567\n",
    "    LinkedIn: linkedin.com/in/janedoe\n",
    "\n",
    "    Summary:\n",
    "    Highly motivated and results-oriented Machine Learning Engineer with 4 years of experience in developing and deploying ML solutions. Proven ability to work with large datasets and implement complex models. Seeking a challenging role at a leading tech company like Google.\n",
    "\n",
    "    Experience:\n",
    "    Senior ML Engineer, Tech Innovations Inc., 2020 - Present\n",
    "    - Led the development of a recommendation system using TensorFlow, resulting in a 15% increase in user engagement.\n",
    "    - Built and maintained data pipelines for processing large-scale datasets.\n",
    "    - Collaborated with product teams to integrate ML models into production systems.\n",
    "\n",
    "    ML Engineer, Data Solutions Ltd., 2018 - 2020\n",
    "    - Developed and deployed machine learning models for fraud detection.\n",
    "    - Worked with cloud platforms (AWS) to manage and scale ML workloads.\n",
    "    - Participated in code reviews and mentored junior engineers.\n",
    "\n",
    "    Education:\n",
    "    Master of Science in Computer Science, Stanford University, 2018\n",
    "    Bachelor of Science in Computer Science, University of California, Berkeley, 2016\n",
    "\n",
    "    Skills:\n",
    "    Programming Languages: Python, Java, C++\n",
    "    ML Frameworks: TensorFlow, PyTorch, scikit-learn\n",
    "    Cloud Platforms: Google Cloud, AWS\n",
    "    Other: SQL, Docker, Kubernetes\n",
    "    \"\"\"\n",
    "\n",
    "    with open('./content/job_posting.txt', 'w') as f:\n",
    "        f.write(job_posting_content)\n",
    "\n",
    "    with open('./content/applicant_cv.txt', 'w') as f:\n",
    "        f.write(applicant_cv_content)\n",
    "\n",
    "    print(\"Re-created '/content/job_posting.txt' and '/content/applicant_cv.txt' with sample content.\")\n",
    "\n",
    "    # Now, attempt to load the files and perform the analysis using Gemini\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    job_posting_path = './content/job_posting.txt'\n",
    "    applicant_cv_path = './content/applicant_cv.txt'\n",
    "\n",
    "    if os.path.exists(job_posting_path) and os.path.exists(applicant_cv_path):\n",
    "        try:\n",
    "            with open(job_posting_path, 'r') as f:\n",
    "                job_posting_text = f.read()\n",
    "\n",
    "            with open(applicant_cv_path, 'r') as f:\n",
    "                applicant_cv_text = f.read()\n",
    "\n",
    "            # Re-define company_research_results\n",
    "            company_name_match = re.search(r\"Company: (.+)\", job_posting_text)\n",
    "            if company_name_match:\n",
    "                company_name = company_name_match.group(1).strip()\n",
    "                company_research_results = f\"Simulated research results for {company_name}: Company is a major tech company known for innovation in AI and search. Culture is fast-paced and collaborative. Additional responsibilities for ML Engineers often involve cross-team collaboration and contributing to open-source projects.\"\n",
    "            else:\n",
    "                company_name = \"the company\"\n",
    "                company_research_results = \"Simulated research results: General information about tech companies.\"\n",
    "\n",
    "\n",
    "            print(\"Performing analysis and comparison of job posting, CV, and company research using Gemini...\")\n",
    "\n",
    "            # Construct a prompt for the gemini_model\n",
    "            prompt = f\"\"\"You are an AI interview assistant. Analyze the following job posting, applicant CV, and company research results to identify:\n",
    "            1. Matching skills between the job posting and the CV.\n",
    "            2. Relevant experience from the CV that aligns with the job posting.\n",
    "            3. Potential cultural fit notes based on the company research and CV.\n",
    "            4. Identified gaps in skills or experience based on the job posting and CV, considering bonus qualifications and company research.\n",
    "\n",
    "            Provide the results in a structured format with clear headings for each category:\n",
    "\n",
    "            Job Posting:\n",
    "            {job_posting_text}\n",
    "\n",
    "            Applicant CV:\n",
    "            {applicant_cv_text}\n",
    "\n",
    "            Company Research Results:\n",
    "            {company_research_results}\n",
    "\n",
    "            Analysis Results:\n",
    "            \"\"\"\n",
    "\n",
    "            # Call the gemini_model to generate the analysis results\n",
    "            # Use a try-except block to handle potential errors during model inference\n",
    "            try:\n",
    "                response = gemini_model.generate_content(prompt)\n",
    "                analysis_text = response.text\n",
    "                print(\"\\nRaw Analysis Results from Gemini:\")\n",
    "                print(analysis_text)\n",
    "\n",
    "                # Parse the model's response text to extract the analysis results\n",
    "                # This is a basic parsing based on expected headings. More robust parsing might be needed.\n",
    "                analysis_results = {}\n",
    "                current_category = None\n",
    "                for line in analysis_text.split('\\n'):\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\"Matching skills:\"):\n",
    "                        current_category = \"matching_skills\"\n",
    "                        analysis_results[current_category] = []\n",
    "                    elif line.startswith(\"Relevant experience:\"):\n",
    "                        current_category = \"relevant_experience\"\n",
    "                        analysis_results[current_category] = []\n",
    "                    elif line.startswith(\"Potential cultural fit notes:\"):\n",
    "                        current_category = \"cultural_fit_notes\"\n",
    "                        analysis_results[current_category] = \"\"\n",
    "                    elif line.startswith(\"Identified gaps:\"):\n",
    "                        current_category = \"identified_gaps\"\n",
    "                        analysis_results[current_category] = []\n",
    "                    elif current_category:\n",
    "                        if current_category in [\"matching_skills\", \"relevant_experience\", \"identified_gaps\"]:\n",
    "                            if line.startswith(\"- \"):\n",
    "                                analysis_results[current_category].append(line[2:].strip())\n",
    "                        elif current_category == \"cultural_fit_notes\":\n",
    "                            analysis_results[current_category] += line.strip() + \" \"\n",
    "\n",
    "                print(\"\\nParsed Analysis Results:\")\n",
    "                for key, value in analysis_results.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during Gemini analysis: {e}\")\n",
    "                # Define a placeholder in case of error to allow subsequent cells to run\n",
    "                analysis_results = {\n",
    "                    \"matching_skills\": [\"Analysis failed\"],\n",
    "                    \"relevant_experience\": [\"Analysis failed\"],\n",
    "                    \"cultural_fit_notes\": \"Analysis failed\",\n",
    "                    \"identified_gaps\": [\"Analysis failed\"]\n",
    "                }\n",
    "                print(\"\\nUsing placeholder analysis_results due to error.\")\n",
    "\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: File not found after re-creation. This is unexpected.\")\n",
    "            job_posting_text = None\n",
    "            applicant_cv_text = None\n",
    "            company_research_results = None\n",
    "            analysis_results = None # Ensure analysis_results is None if file loading fails\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the files or performing initial steps: {e}\")\n",
    "            job_posting_text = None\n",
    "            applicant_cv_text = None\n",
    "            company_research_results = None\n",
    "            analysis_results = None # Ensure analysis_results is None if error occurs\n",
    "    else:\n",
    "        print(\"Error: Files do not exist in the specified directory after re-creation. This is unexpected.\")\n",
    "        job_posting_text = None\n",
    "        applicant_cv_text = None\n",
    "        company_research_results = None\n",
    "        analysis_results = None # Ensure analysis_results is None if files don't exist\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Gemini model initialization: {e}\")\n",
    "    job_posting_text = None\n",
    "    applicant_cv_text = None\n",
    "    company_research_results = None\n",
    "    analysis_results = None # Ensure analysis_results is None if model initialization fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc79f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming analysis_results and gemini_model are available from previous steps.\n",
    "\n",
    "print(\"Generating interview questions using Gemini based on analysis results...\")\n",
    "\n",
    "# Define a prompt for the gemini_model\n",
    "prompt = f\"\"\"You are an AI interview assistant. Based on the following analysis of a job posting, applicant CV, and company research, generate a list of tailored interview questions.\n",
    "Focus on creating questions that probe the applicant's strengths in matching areas and explore the identified gaps. Also, include questions related to cultural fit based on the company research.\n",
    "Provide only the list of questions, one per line, starting with a number.\n",
    "\n",
    "Analysis Results:\n",
    "Matching skills: {analysis_results.get('matching_skills', 'N/A')}\n",
    "Relevant experience: {analysis_results.get('relevant_experience', 'N/A')}\n",
    "Cultural fit notes: {analysis_results.get('cultural_fit_notes', 'N/A')}\n",
    "Identified gaps: {analysis_results.get('identified_gaps', 'N/A')}\n",
    "\n",
    "Generate Interview Questions:\n",
    "\"\"\"\n",
    "\n",
    "simulated_interview_questions = []\n",
    "\n",
    "# Call the gemini_model to generate the interview questions\n",
    "# Use a try-except block to handle potential errors during model inference\n",
    "try:\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    questions_text = response.text\n",
    "    print(\"\\nRaw Generated Questions from Gemini:\")\n",
    "    print(questions_text)\n",
    "\n",
    "    # Parse the model's response text to extract the interview questions\n",
    "    # Assuming each question is on a new line and starts with a number\n",
    "    for line in questions_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith('-')): # Basic check for numbered or bulleted list\n",
    "             simulated_interview_questions.append(line.split('. ', 1)[-1].strip() if line[0].isdigit() else line[2:].strip())\n",
    "\n",
    "\n",
    "    if not simulated_interview_questions:\n",
    "         print(\"\\nWarning: No questions were parsed from the model's response. Using default questions.\")\n",
    "         # Assign a default list of questions if parsing failed\n",
    "         simulated_interview_questions = [\n",
    "            \"Tell me about yourself.\",\n",
    "            \"Why are you interested in this role?\",\n",
    "            \"What are your strengths and weaknesses?\",\n",
    "            \"Do you have any questions for me?\"\n",
    "        ]\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Gemini question generation: {e}\")\n",
    "    # Assign a default list of questions in case of error\n",
    "    simulated_interview_questions = [\n",
    "        \"Tell me about yourself.\",\n",
    "        \"Why are you interested in this role?\",\n",
    "        \"What are your strengths and weaknesses?\",\n",
    "        \"Do you have any questions for me?\"\n",
    "    ]\n",
    "    print(\"\\nUsing default interview questions due to error.\")\n",
    "\n",
    "\n",
    "print(\"\\nSimulated Interview Questions:\")\n",
    "for i, question in enumerate(simulated_interview_questions):\n",
    "    print(f\"{i+1}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming simulated_interview_questions, speak_question, listen_to_applicant,\n",
    "# simulated_score, and hiring_decision variables are defined in previous cells.\n",
    "\n",
    "print(\"Starting the simulated interview...\")\n",
    "\n",
    "# AI Introduction\n",
    "intro_message = \"Hello, I am your AI interviewer. I will be asking you a series of questions about your background and experience to assess your suitability for the role. Please respond verbally after each question. The interview will take approximately 15 minutes. Do you have any questions before we begin?\"\n",
    "speak_question(intro_message)\n",
    "\n",
    "# In a real application, you might listen for a response here to the intro question.\n",
    "# For this simulation, we'll proceed directly to the interview questions.\n",
    "applicant_intro_response = listen_to_applicant() # Uncomment in a real scenario\n",
    "\n",
    "interview_responses = {}\n",
    "\n",
    "for i, question in enumerate(simulated_interview_questions):\n",
    "    print(f\"\\nAsking question {i+1}: {question}\")\n",
    "    speak_question(question)\n",
    "\n",
    "    print(\"Waiting for applicant response...\")\n",
    "    # In a real application, you would uncomment the line below:\n",
    "    applicant_response = listen_to_applicant() # Uncomment this in a real interactive scenario\n",
    "    # For simulation purposes, you might use a placeholder if not running interactively\n",
    "    # applicant_response = f\"Simulated response to question {i+1}\"\n",
    "\n",
    "    interview_responses[question] = applicant_response\n",
    "    print(f\"Applicant response recorded: {applicant_response}\")\n",
    "\n",
    "print(\"\\nSimulated interview concluded.\")\n",
    "print(\"\\nInterview Responses:\")\n",
    "for question, response in interview_responses.items():\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Simulated applicant thanking the interviewer\n",
    "print(\"\\nApplicant thanking the interviewer (simulated)...\")\n",
    "# In a real application, this would be a response captured by listen_to_applicant()\n",
    "simulated_thank_you = \"Thank you for your time today. I enjoyed learning more about this opportunity.\"\n",
    "# If you had the interviewer's name, you could add it here, e.g., \"Thank you, [Interviewer Name], for your time today.\"\n",
    "print(f\"Simulated applicant says: {simulated_thank_you}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini API - necessary for this step\n",
    "# import google.generativeai as genai\n",
    "# from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    # GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "    # genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    # gemini_model = genai.GenerativeModel('gemini-pro') # Or another suitable model\n",
    "\n",
    "    print(\"Gemini model initialized successfully for scoring.\")\n",
    "\n",
    "    # Define placeholder simulated_interview_questions and interview_responses if they are not already defined\n",
    "    if 'simulated_interview_questions' not in locals() or not simulated_interview_questions:\n",
    "        simulated_interview_questions = [\n",
    "            \"Your CV highlights experience with TensorFlow and PyTorch. Can you describe a project where you used one of these frameworks to build a scalable ML model?\",\n",
    "            \"You've built and maintained data pipelines. What are some of the key challenges you've faced in this area and how did you address them?\",\n",
    "            \"The job posting mentions collaborating with cross-functional teams. Can you give an example of a time you successfully collaborated with a non-ML team?\",\n",
    "            \"Our company culture values collaboration and contributions to the wider tech community. How do you approach collaboration, and have you contributed to any open-source projects?\",\n",
    "            \"The job posting listed natural language processing and computer vision as bonus qualifications. Do you have any experience or interest in these areas?\",\n",
    "            \"Publications in top-tier ML conferences were also listed as a bonus. Have you published any research, or are you interested in pursuing publications?\",\n",
    "            \"Based on your understanding of Google's work in AI, what kind of ML projects are you most excited about working on?\",\n",
    "            \"Do you have any questions for me about the role or the company?\"\n",
    "        ]\n",
    "        print(\"Using placeholder simulated_interview_questions.\")\n",
    "\n",
    "    if 'interview_responses' not in locals() or not interview_responses:\n",
    "        # Create simulated responses for the placeholder questions\n",
    "        interview_responses = {\n",
    "            simulated_interview_questions[0]: \"Simulated response to question 1: I worked on a recommendation system using TensorFlow that scaled to millions of users.\",\n",
    "            simulated_interview_questions[1]: \"Simulated response to question 2: Challenges included data inconsistency, which I addressed by implementing robust validation checks.\",\n",
    "            simulated_interview_questions[2]: \"Simulated response to question 3: I collaborated with the marketing team to integrate an ML model for targeted advertising.\",\n",
    "            simulated_interview_questions[3]: \"Simulated response to question 4: I believe in open communication and have contributed to a small open-source data processing library.\",\n",
    "            simulated_interview_questions[4]: \"Simulated response to question 5: I have some academic experience in NLP and am very interested in learning more about CV.\",\n",
    "            simulated_interview_questions[5]: \"Simulated response to question 6: I haven't published yet, but it's a goal I'm actively working towards.\",\n",
    "            simulated_interview_questions[6]: \"Simulated response to question 7: I'm most excited about Google's work in responsible AI and applying ML to solve real-world problems.\",\n",
    "            simulated_interview_questions[7]: \"Simulated response to question 8: Yes, can you describe the typical day-to-day of an ML Engineer on your team?\"\n",
    "        }\n",
    "        print(\"Using placeholder interview_responses.\")\n",
    "\n",
    "\n",
    "    print(\"\\nProcessing interview responses and calculating score using Gemini...\")\n",
    "\n",
    "    # Construct a detailed prompt for Gemini to analyze the interview responses\n",
    "    scoring_prompt = f\"\"\"You are an AI interview assistant. Evaluate the following interview responses based on the provided job posting and interview questions.\n",
    "    Assess the applicant's responses for:\n",
    "    - Relevance to the questions and job requirements.\n",
    "    - Technical accuracy and depth (where applicable).\n",
    "    - Communication skills (clarity, conciseness).\n",
    "    - Alignment with the company culture and values (based on the simulated company research provided earlier in the process).\n",
    "\n",
    "    Provide a score out of 100 and a hiring recommendation.\n",
    "\n",
    "    Job Posting:\n",
    "    {job_posting_text if 'job_posting_text' in locals() and job_posting_text else 'N/A'}\n",
    "\n",
    "    Interview Questions and Responses:\n",
    "    \"\"\"\n",
    "    for question, response in interview_responses.items():\n",
    "        scoring_prompt += f\"Q: {question}\\nA: {response}\\n---\\n\"\n",
    "\n",
    "    scoring_prompt += \"\"\"\n",
    "\n",
    "    Based on this evaluation, provide a score (out of 100) and a hiring recommendation. Format your response clearly, like this:\n",
    "\n",
    "    Score: [Score out of 100]\n",
    "    Recommendation: [Hiring Recommendation]\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the gemini_model to generate the score and recommendation\n",
    "    gemini_score = None\n",
    "    gemini_hiring_decision = \"Could not get recommendation from AI.\"\n",
    "\n",
    "  \n",
    "    try:\n",
    "        response = gemini_model.generate_content(scoring_prompt)\n",
    "        scoring_text = response.text\n",
    "        print(\"\\nRaw Scoring Results from Gemini:\")\n",
    "        print(scoring_text)\n",
    "\n",
    "        # Parse the model's response to extract the score and recommendation\n",
    "        score_match = re.search(r\"Score: (\\d+)\", scoring_text)\n",
    "        if score_match:\n",
    "            gemini_score = int(score_match.group(1))\n",
    "\n",
    "        recommendation_match = re.search(r\"Recommendation: (.+)\", scoring_text)\n",
    "        if recommendation_match:\n",
    "            gemini_hiring_decision = recommendation_match.group(1).strip()\n",
    "        else:\n",
    "            # Fallback: if we have a score but no explicit recommendation, derive one from the score\n",
    "            if gemini_score is not None:\n",
    "                hiring_threshold = 70\n",
    "                gemini_hiring_decision = (\"Candidate meets criteria.\" \n",
    "                                          if gemini_score >= hiring_threshold \n",
    "                                          else \"Candidate does not meet criteria.\")\n",
    "            # otherwise keep the default gemini_hiring_decision\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini scoring: {e}\")\n",
    "        print(\"Using fallback scoring due to AI error.\")\n",
    "\n",
    "\n",
    "    # Use Gemini's results if available, otherwise use the basic placeholder logic\n",
    "    if gemini_score is not None:\n",
    "        simulated_score = gemini_score\n",
    "        hiring_decision = gemini_hiring_decision\n",
    "        print(\"\\nUsing Gemini's score and recommendation.\")\n",
    "    else:\n",
    "        # Fallback to basic placeholder scoring if Gemini call failed\n",
    "        num_questions = len(simulated_interview_questions)\n",
    "        num_responses = len(interview_responses)\n",
    "        simulated_score = int((num_responses / num_questions) * 100) if num_questions > 0 else 0\n",
    "        hiring_decision = \"Could not get AI recommendation. Fallback decision: \"\n",
    "        hiring_threshold = 70 # Example threshold\n",
    "        if simulated_score >= hiring_threshold:\n",
    "             hiring_decision += \"Candidate meets basic criteria.\"\n",
    "        else:\n",
    "             hiring_decision += \"Candidate does not meet basic criteria.\"\n",
    "        print(\"\\nUsing fallback score and recommendation.\")\n",
    "\n",
    "\n",
    "    print(f\"\\nApplicant's simulated interview score: {simulated_score}\")\n",
    "    print(f\"Hiring Decision: {hiring_decision}\")\n",
    "    conclusion_message = f\"Thank you for completing the interview. The interview has now concluded. Your simulated score is {simulated_score}. The hiring recommendation is: {hiring_decision}.\"\n",
    "    speak_question(conclusion_message)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Gemini model initialization for scoring: {e}\")\n",
    "    print(\"\\nSkipping scoring and decision due to model initialization error.\")\n",
    "    # Provide a placeholder score and decision if model initialization fails\n",
    "    simulated_score = \"N/A\"\n",
    "    hiring_decision = \"Could not perform scoring due to AI model error.\"\n",
    "    print(f\"\\nApplicant's simulated interview score: {simulated_score}\")\n",
    "    print(f\"Hiring Decision: {hiring_decision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690445a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
